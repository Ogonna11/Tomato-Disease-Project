---
title: "Compositional Data Analysis"
output:
  html_document: default
     
date: "2023-12-28"
---

```{r, echo=FALSE}
setwd("/home/ogonna/Desktop/Lenovo Laptop/Tomatoes Projects/Latest_data_and_codes")
```



#  Load necessary libraries.
```{r, message=FALSE}
library(compositions)  # For compositional data analysis

```



# 1. Data Preparation
```{r}
data <- read.csv("Model.csv")
data<-data[,-1]
Response <- data[, c("SC1", "SC2", "SC3", "SC4", "SC5", "SC6", "SC7", "SC8")]
data <- data[, !(names(data) %in% c("SC1", "SC2", "SC3", "SC4", "SC5", "SC6", "SC7", "SC8"))]

# Treat Response SC1 to SC8 as composition
Y <- acomp(Response)


# Convert categorical variables to factors and introduce contrast 
Time <- factor(data$Time)
Year<-factor(data$Year)

Severity <- factor(data$Severity, ordered = TRUE)   
Commercial_small_scale <- factor(data$Commercial.small.scale,c("Small", "Commercial"), ordered = TRUE)


contrasts(Severity)<-"contr.treatment"
contrasts(Commercial_small_scale)<-"contr.treatment"
contrasts(Time)<-"contr.treatment"
contrasts(Year)<-"contr.treatment"

data$Year<-Year
data$Severity<-Severity
data$Commercial.small.scale<-Commercial_small_scale
data$Time<-Time

```


# 2. Compositional Response Regression Model 

**2.1. With continuous predictor (Regression).**

Regression model with compositional response and real independent variable is defined using the linear Aitchison structure of the simplex:

$$Y_i = a \oplus   X_{i} \odot b  \oplus  \varepsilon_{i},$$


where $a$ and $b$ are unknown compositional constants, $Y_i$ is a random composition,
$X_i$ is a real random variable or design constant, and finally $\varepsilon_i$ is a compositional
random variable with null compositional expectation and a constant variance.



**Interpretation:**
The intercept $a$ is the expected composition for X = 0.The slope $b$ may be interpreted as the perturbation applied to the composition if $X$ increases one unit.

**2.2. With categorical predictor (Compositional Analysis of Variance).**

Formally, the compositional analysis of variance model uses the same equation as
in regression,


$$Y_i = a \oplus b_g({X_{i}})  \oplus  \varepsilon_{i},$$


where $a$ and $b_g$, $g\in \{g_1, \dots, g_m\}$ are unknown compositional constants, $Y_i$ is a random composition,
$X_i$ is a real random variable or design constant, and finally $\varepsilon_i$ is a compositional
random variable normally distributed with null compositional expectation and a constant variance.



**Interpretation:**
Since the ANOVA models have `identifiability` problem and that we must fix the contrasts we want to use for our factors. In general, we will prefer the treatment contrast. In that way, we will be able to interpret the:

1.) Intercept `a` as the expected response for the first level and,

2.) the remaining constants $b_{g_2}, b_{g_3}, \dots$ as the increments on average response from the first to the second level, from the first to the third, etc.


Thus, we have several compositional constants $a, b_{g_2}, b_{g_3}, \dots,$ a different one for
each group $g_i, i = 1, \dots, m$.


**2.3. Linear Models with Several Main Effects.**

Multiple covariables can be added into a linear model simply by adding several of
the influence terms into the modeling equation:

$$Y_i = a \oplus  X1_{i} \odot b_1  \oplus  X2_{i} \odot b_2  \oplus b_3({X3_{i}}) \oplus \dots \oplus \varepsilon_{i}.$$

 # 3. Implementation in R
 
 We fit compostional linear model using some predictors from our dataset with SC1 to SC8 as the compositional response.
 
```{r}
# Get all predictor variable names
predictor_vars <- names(data)

# Create the formula string
formula_str <- paste("ilr(Y) ~", paste(predictor_vars, collapse = " + "))

# Convert the string to a formula
formula <- as.formula(formula_str)

```
 
```{r}
model <- lm(formula, data = data)
model
```



```{r}

#frm <- as.formula(paste0(names(ilr(Y)), "~", paste0(unlist(names(data)[1:ncol(data[,-c(6:13)])]), collapse = "+")))

#model = lm(ilr(Y) ~ data$Time + data$Year + log(data$Sd_WD10M) + log(data$Sd_WS10M) + data$Skew_ALLSKY_SFC_LW_DWN + data$Abs +data$Rel +data$shannon_diversity + data$Commercial.small.scale
 #           + data$Skew_PS + data$Sd_PS + data$Kur_T2M + data$Kur_TS  +  data$Kur_ALLSKY_SFC_LW_DWN +
 #             data$Kur_CLRSKY_SFC_PAR_TOT + data$Kur_WS10M+data$Kur_PRECTOTCORR+data$Severity)

#model <- lm(frm, data = data)

#model
```


**Remarks and Comments.**

1.) Log transform should be applied to ratio scaled continuous predictors. We need to identify such variables in our data.

2.) We need to find a way of doing variable selection in this setting, alternatively we seek how to manage high dimensional data in this setting.

3.)  The output contains the estimated parameters in Isometric Log-Ratio (ilr) coordinates: the intercept
$\hat{a}$ , the slopes $\hat{b}$ for each continuous covariable, and the ANOVA effects $\hat{b}(X)$, with one row for each level beyond the first. The first level of the factor is assumed to have an effect of zero due to the implicit usage of the `contr.treatment` contrasts.



# Estimated compositional parameter (Coefficients).

The output contains the estimated parameters $a$ (intercept) and $b$ (slope) in ilr coordinates. They
can be displayed as compositions by:

 1.) Intercept.
```{r}
(a = ilrInv(coef(model)[1,],orig=Y))
```

2.) Slope.

```{r}
(b = ilrInv(rbind(0,coef(model)[-1,]),orig=Y))

```


# Visualizing the model

The parameters can graphically be represented as compositions. Below is a bar chart representation of the parameters of the model  with compositional response and several main effects. Each bar is associated to a continuous predictor or level of a categorical predictors.

```{r}
coefs <- ilrInv(coef(model),orig=Y)
# Define a color palette
colors <- rainbow(ncol(coefs))

# Create the bar plot with colors
barplot(coefs, las=2, xlim=c(0, 95), col=colors)
```

# Testing the Influence of Each Variable


The output of the `summary` command for a truly multivariate model in ilr coordinates does not make much sense, as it tests each coordinate separately and not as a whole. However, we can use the `anova.mlm` to check for the joint significance of the influence of $X$; precisely the `anova` command is used to test that the role of each variable is significant:



```{r}
anova(model)
```

**Interpretation:**

Each line of the table corresponds to a test of the form:

$H_0$, null hypothesis: given the preceding $(i-1)$ variables, the $ith$ predictor
has no influence. If this is a continuous covariable, we can say that the slope
coefficient $b_i=\mathbb{1}$. If this is a factor, we can say that the composition has the
same expectation for all levels.


$H_1$, alternative hypothesis: the independent variable of the line is needed,
additionally to all preceding variables, i.e., its coefficients $b_i\neq\mathbb{1}$.


**Comment:**

A variable is only needed in the model if it is necessary given that the information
from all the other covariables is already used. It is thus necessary to check each
covariable as last term in the model. Currently, there is no way to automatize this, since the built-in R command `drop1` is not functional for the "`mlm`" model. We need to manually enumerate each
covariable in the last position and extract the test of the last row of the ANOVA table.


We need to do lasso one way or the other or apply some variable selection techniques in this setting.
